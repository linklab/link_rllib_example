{"train": 78, "episodes": 937, "timesteps": 312000, "optimizations": 9984, "train/episode_reward_mean": 480.89, "train/episode_reward_min": 230.0, "train/episode_reward_max": 500.0, "train/episode_length_mean": 480.89, "evaluation/episode_reward_mean": 500.0, "evaluation/episode_reward_min": 500.0, "evaluation/episode_reward_max": 500.0, "evaluation/episode_length_mean": 500.0, "loss": 2.8246783157830597, "_timestamp": 1667733411.43485, "_runtime": 383.1184549331665, "_step": 77}